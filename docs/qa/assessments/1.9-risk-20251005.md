# Risk Profile: Story 1.9

Date: 2025-10-05
Reviewer: Quinn (Test Architect)

## Executive Summary

- Total Risks Identified: 8
- Critical Risks: 1
- High Risks: 2
- Risk Score: 71/100 (moderate risk)

## Critical Risks Requiring Immediate Attention

### 1. SEC-001: Production Error Message Information Leakage

**Score: 9 (Critical)**
**Probability**: High (3) - Environment detection is critical security control that must work correctly
**Impact**: High (3) - Production error messages leaking stack traces or sensitive details could expose system internals to attackers

**Mitigation**:

- Implement comprehensive environment-specific tests (1.9-UNIT-010, 1.9-UNIT-011)
- Verify production mode returns generic error messages
- Test logging behavior differs between development and production (1.9-UNIT-012, 1.9-UNIT-013)
- Use vi.stubEnv with proper cleanup to ensure reliable environment detection
- Manual security review of error handler logic before deployment

**Testing Focus**:

- Both development and production environment configurations
- Stack trace presence/absence verification
- Error message content validation
- Environment stub cleanup verification

## Risk Distribution

### By Category

- Security: 2 risks (1 critical)
- Technical: 3 risks (1 high)
- Operational: 2 risks (1 high)
- Performance: 1 risk (0 critical)
- Data: 0 risks
- Business: 0 risks

### By Component

- Error Handler: 5 risks (lib/api/errorHandler.ts)
- Health Endpoint: 1 risk (app/api/health/route.ts)
- Header Component: 2 risks (components/layout/Header.tsx)

## Detailed Risk Register

| Risk ID  | Category    | Description                                       | Probability | Impact     | Score | Mitigation Status |
| -------- | ----------- | ------------------------------------------------- | ----------- | ---------- | ----- | ----------------- |
| SEC-001  | Security    | Production error message information leakage      | High (3)    | High (3)   | 9     | Test coverage     |
| TECH-001 | Technical   | Test helper mismatch with actual error structures | Medium (2)  | High (3)   | 6     | Version matching  |
| OPS-001  | Operational | Test flakiness from async state management        | Medium (2)  | High (3)   | 6     | waitFor patterns  |
| TECH-002 | Technical   | Environment stub cleanup failures                 | Medium (2)  | Medium (2) | 4     | afterEach cleanup |
| SEC-002  | Security    | Missing error type in handler                     | Low (1)     | Medium (2) | 2     | Comprehensive AC  |
| TECH-003 | Technical   | React testing async race conditions               | Low (1)     | Medium (2) | 2     | Testing patterns  |
| OPS-002  | Operational | Coverage target (95%+) maintenance burden         | Low (1)     | Medium (2) | 2     | Tooling support   |
| PERF-001 | Performance | Console mocking test execution overhead           | Low (1)     | Low (1)    | 1     | Acceptable        |

## Detailed Risk Analysis

### SEC-001: Production Error Message Information Leakage

**Category**: Security
**Affected Components**: lib/api/errorHandler.ts
**Detection Method**: Security requirements analysis

**Description**: The error handler must prevent information disclosure in production environments by returning generic error messages and omitting stack traces. If environment detection fails or is misconfigured, sensitive system details could be exposed to end users or attackers.

**Probability**: High (3)

- Environment variable detection is complex
- NODE_ENV misconfiguration is common in deployments
- Test environment stubbing can create false confidence

**Impact**: High (3)

- Stack traces reveal system architecture and file paths
- Detailed error messages expose database structure
- Security through obscurity compromised
- Potential compliance violations (PCI DSS, GDPR)

**Mitigation Strategy**: Preventive
**Actions**:

1. Implement environment-specific error message tests (AC: 8)
   - Test development environment returns actual error messages (1.9-UNIT-010)
   - Test production environment returns generic messages (1.9-UNIT-011)
   - Verify message content exactly matches expected strings
2. Implement environment-specific logging tests (AC: 10)
   - Test development includes stack traces (1.9-UNIT-012)
   - Test production excludes stack traces (1.9-UNIT-013)
   - Mock console.error and verify logged data structure
3. Use vi.stubEnv('NODE_ENV', 'production') with vi.unstubAllEnvs() cleanup
4. Manual security code review before deployment
5. Add integration test with production-like configuration

**Testing Requirements**:

- 4 dedicated test scenarios (1.9-UNIT-010 through 1.9-UNIT-013)
- Mock verification for console.error calls
- Environment stub cleanup verification
- Manual security testing in staging with production config

**Residual Risk**: Low - Some zero-day vulnerabilities or unexpected error types may still leak information
**Owner**: Dev (implementation) → QA (validation) → Security Review
**Timeline**: Before production deployment (critical gate requirement)

---

### TECH-001: Test Helper Mismatch with Actual Error Structures

**Category**: Technical
**Affected Components**: lib/api/errorHandler.test.ts (test helpers)
**Detection Method**: Technical analysis of test patterns

**Description**: Test helpers (createPrismaError, createZodError) must accurately replicate the structure of real errors from external libraries. If helpers don't match actual error structures, tests may pass but production code fails.

**Probability**: Medium (2)

- Prisma error constructor requires exact clientVersion match
- ZodError detection uses name property, not instanceof
- External library versions may change error structures

**Impact**: High (3)

- False positive test results
- Production failures despite passing tests
- Debugging difficulty when production differs from tests
- Loss of confidence in test suite

**Mitigation Strategy**: Preventive
**Actions**:

1. Create createPrismaError helper matching exact constructor signature
   - Use Prisma.PrismaClientKnownRequestError from @prisma/client
   - Match clientVersion to package.json version (5.22.0)
   - Include required properties: code, clientVersion, meta (optional)
2. Create createZodError helper with correct name property
   - Set error.name = 'ZodError' (not instanceof check)
   - Match error handler detection logic exactly
3. Add verification test for Prisma clientVersion match
4. Document helper requirements in test file
5. Add CI check to validate @prisma/client version consistency

**Testing Requirements**:

- Verify helper in first test (1.9-UNIT-001 for Prisma)
- Verify helper in first test (1.9-UNIT-004 for ZodError)
- Document version requirements in test comments
- Add version check as explicit test assertion

**Residual Risk**: Low - Library updates could change error structures without warning
**Owner**: Dev (test implementation)
**Timeline**: During test implementation (Task 1, Task 2)

---

### OPS-001: Test Flakiness from Async State Management

**Category**: Operational
**Affected Components**: components/layout/Header.test.tsx
**Detection Method**: React testing best practices analysis

**Description**: React component tests involving asynchronous state updates (loading states, API calls) can exhibit race conditions and flakiness if not properly synchronized using waitFor() patterns.

**Probability**: Medium (2)

- React state updates are asynchronous by nature
- 6 Header component tests involve async behavior
- fireEvent triggers async state changes
- Common source of test flakiness in React apps

**Impact**: High (3)

- Intermittent test failures block CI/CD pipeline
- Developer time wasted investigating flaky tests
- Reduced confidence in test suite
- Potential to mask real bugs if tests disabled

**Mitigation Strategy**: Preventive
**Actions**:

1. Use waitFor() for all async state assertions
   - Button loading state changes (1.9-UNIT-019, 1.9-UNIT-020)
   - Console.error verification (1.9-UNIT-015, 1.9-UNIT-016)
   - State reset in finally block (1.9-UNIT-017)
2. Avoid testing state immediately after fireEvent
3. Set appropriate timeout values in waitFor (default: 1000ms)
4. Use screen queries that wait by default (findBy* instead of getBy*)
5. Mock global.fetch before render to prevent unmocked API calls
6. Clear all mocks in beforeEach for test isolation

**Testing Requirements**:

- All 6 Header tests use waitFor() for async assertions
- Verify test passes 10 consecutive runs locally
- Run tests in CI with --run-in-band to catch timing issues
- Document async patterns in test file comments

**Residual Risk**: Low - Some race conditions may only appear under specific CPU load
**Owner**: Dev (test implementation) → QA (validation)
**Timeline**: During test implementation (Task 8, Task 9)

---

### TECH-002: Environment Stub Cleanup Failures

**Category**: Technical
**Affected Components**: lib/api/errorHandler.test.ts
**Detection Method**: Vitest environment stubbing best practices

**Description**: vi.stubEnv modifies process.env globally. If cleanup (vi.unstubAllEnvs) is not called in afterEach, environment stubs leak to subsequent tests causing unpredictable failures and test order dependencies.

**Probability**: Medium (2)

- Easy to forget cleanup in new test files
- Not obvious when looking at individual tests
- May only manifest when test order changes

**Impact**: Medium (2)

- Intermittent test failures based on execution order
- Debugging complexity (works alone, fails in suite)
- False test results from wrong environment
- CI failures that don't reproduce locally

**Mitigation Strategy**: Preventive
**Actions**:

1. Add afterEach(() => vi.unstubAllEnvs()) at describe block level
2. Place cleanup immediately after environment tests for visibility
3. Verify cleanup in test review checklist
4. Run tests with --run-in-band to catch order dependencies
5. Document pattern in test file header comment

**Testing Requirements**:

- Verify cleanup present in error handler test file
- Run tests in different orders to validate independence
- Include in code review checklist

**Residual Risk**: Low - Pattern is well-established and easy to verify
**Owner**: Dev (test implementation)
**Timeline**: During test implementation (Task 4)

---

### SEC-002: Missing Error Type in Handler

**Category**: Security
**Affected Components**: lib/api/errorHandler.ts
**Detection Method**: Acceptance criteria completeness analysis

**Description**: If error handler doesn't cover all possible error types, unexpected errors could leak information or return incorrect HTTP status codes.

**Probability**: Low (1)

- Story ACs are comprehensive (Prisma, ZodError, custom classes, generic)
- Error handler has fallback for unknown types
- Prior stories established error patterns

**Impact**: Medium (2)

- Information leakage for uncovered error types
- Incorrect HTTP status codes confuse clients
- Poor user experience for unhandled errors

**Mitigation Strategy**: Preventive
**Actions**:

1. Test all documented error types (14 error handler scenarios)
2. Test non-Error object fallback (1.9-UNIT-014)
3. Review error handler code for unhandled branches
4. Document assumptions about error type coverage

**Testing Requirements**:

- 14 error handler tests cover all AC scenarios
- Non-Error object test validates fallback
- Code coverage report shows 95%+ coverage

**Residual Risk**: Low - Comprehensive coverage and fallback logic
**Owner**: Dev (implementation and tests)
**Timeline**: During implementation and testing

---

### TECH-003: React Testing Async Race Conditions

**Category**: Technical
**Affected Components**: components/layout/Header.test.tsx
**Detection Method**: React Testing Library best practices

**Description**: Testing async state transitions (button disabled → enabled, text changes) can fail if assertions run before state updates complete.

**Probability**: Low (1)

- waitFor() is established pattern
- Story includes detailed testing guidance
- React Testing Library encourages async-aware testing

**Impact**: Medium (2)

- Flaky tests reduce confidence
- May require multiple test runs
- Developer frustration

**Mitigation Strategy**: Preventive
**Actions**:

1. Use waitFor() for all state-dependent assertions
2. Test state transitions in sequence, not simultaneously
3. Mock fetch before component renders
4. Use findBy\* queries when possible

**Testing Requirements**:

- All async assertions use waitFor()
- Document async patterns in test file
- Verify tests pass 10 consecutive runs

**Residual Risk**: Low - Pattern is well-documented and tested
**Owner**: Dev (test implementation)
**Timeline**: During test implementation (Task 9)

---

### OPS-002: Coverage Target (95%+) Maintenance Burden

**Category**: Operational
**Affected Components**: Overall project test coverage
**Detection Method**: Coverage target analysis

**Description**: Maintaining 95%+ coverage long-term requires discipline and may lead to testing diminishing-return code paths or discouraging refactoring.

**Probability**: Low (1)

- Current coverage is 58.93%, target is achievable
- Story focuses on critical error paths
- Coverage tools (Vitest + @vitest/coverage-v8) are robust

**Impact**: Medium (2)

- Difficult to maintain over time
- May discourage beneficial refactoring
- Can lead to testing for coverage rather than value

**Mitigation Strategy**: Detective
**Actions**:

1. Focus on critical paths (error handling, security)
2. Allow exceptions for trivial code paths
3. Regular review of coverage reports
4. Prioritize test value over coverage percentage

**Testing Requirements**:

- Coverage target validation (Task 10)
- HTML coverage report review
- Document intentionally uncovered lines

**Residual Risk**: Medium - Long-term maintenance requires ongoing discipline
**Owner**: QA (monitoring) → Team (maintenance)
**Timeline**: Ongoing after story completion

---

### PERF-001: Console Mocking Test Execution Overhead

**Category**: Performance
**Affected Components**: All test files with console mocks
**Detection Method**: Performance considerations analysis

**Description**: Mocking console.error adds minimal overhead but accumulates across 22 tests.

**Probability**: Low (1)

- Console operations are fast
- Mocking is standard practice
- Total test count (22) is manageable

**Impact**: Low (1)

- Slightly slower test execution
- Negligible impact on developer experience

**Mitigation Strategy**: Accepted Risk
**Actions**:

- No action needed
- Standard practice for preventing test output pollution

**Testing Requirements**:

- Monitor total test execution time
- Target: <10 seconds for all 22 tests

**Residual Risk**: None - Acceptable overhead
**Owner**: N/A
**Timeline**: N/A

## Risk-Based Testing Strategy

### Priority 1: Critical Risk Tests (SEC-001)

**Test Scenarios**:

- 1.9-UNIT-010: Development environment error message (actual message returned)
- 1.9-UNIT-011: Production environment error message (generic message returned)
- 1.9-UNIT-012: Development logging with stack trace
- 1.9-UNIT-013: Production logging without stack trace

**Required Test Types**:

- Unit tests with environment stubbing
- Integration tests with production-like config
- Manual security testing in staging

**Test Data Requirements**:

- Realistic error messages with sensitive information
- Stack trace samples
- Environment configuration variants

**Success Criteria**:

- All 4 environment tests pass
- Manual security review confirms no information leakage
- Production error messages are generic and safe

### Priority 2: High Risk Tests (TECH-001, OPS-001)

**Test Scenarios for TECH-001**:

- 1.9-UNIT-001: P2002 Prisma error (validates helper accuracy)
- 1.9-UNIT-002: P2025 Prisma error
- 1.9-UNIT-003: Unknown Prisma code
- 1.9-UNIT-004: ZodError (validates helper name property)

**Test Scenarios for OPS-001**:

- 1.9-UNIT-015: Fetch network error (async state management)
- 1.9-UNIT-016: Fetch non-ok response
- 1.9-UNIT-017: Finally block state reset
- 1.9-UNIT-018: No redirect on error
- 1.9-UNIT-019: Button disabled during logout
- 1.9-UNIT-020: Button enabled after error

**Integration Test Scenarios**:

- Run Header tests 10 times consecutively
- Run tests with --run-in-band to detect order dependencies
- Monitor for flakiness in CI runs

**Edge Case Coverage**:

- Prisma clientVersion mismatch (should fail test, not production)
- React state update race conditions
- Fetch mock timing variations

### Priority 3: Medium/Low Risk Tests

**Standard Functional Tests**:

- 1.9-UNIT-005 through 1.9-UNIT-009: Custom error classes
- 1.9-UNIT-014: Non-Error object handling
- 1.9-INT-001, 1.9-INT-002: Health endpoint error paths

**Regression Test Suite**:

- Existing 68 tests continue passing
- Overall coverage increases from 58.93% to 95%+

**Success Criteria**:

- All 22 new tests pass
- No regression in existing tests
- Coverage targets achieved

## Risk Acceptance Criteria

### Must Fix Before Production

**Critical Risks (Score 9)**:

- SEC-001: Production error message leakage
  - **Criteria**: All 4 environment tests pass
  - **Criteria**: Manual security review sign-off
  - **Criteria**: Staging environment testing with production NODE_ENV

**High Risks (Score 6)**:

- TECH-001: Test helper accuracy
  - **Criteria**: Prisma clientVersion matches package.json (5.22.0)
  - **Criteria**: First test of each error type validates helper
- OPS-001: Test flakiness
  - **Criteria**: All tests pass 10 consecutive runs
  - **Criteria**: No failures in CI over 5 runs

### Can Deploy with Mitigation

**Medium Risks (Score 4)**:

- TECH-002: Environment stub cleanup
  - **Mitigation**: afterEach cleanup verified in code review
  - **Monitoring**: CI test order randomization

**Low Risks (Score 2-3)**:

- SEC-002, TECH-003, OPS-002
  - **Mitigation**: Comprehensive test coverage
  - **Monitoring**: Coverage reports and test stability metrics

### Accepted Risks

**Minimal Risk (Score 1)**:

- PERF-001: Console mocking overhead
  - **Justification**: Standard practice, negligible impact
  - **Sign-off**: Team consensus (no action needed)

## Monitoring Requirements

Post-deployment monitoring for:

### Security Metrics (SEC-001, SEC-002)

- Error logs in production for information leakage
- Sentry/error tracking for unexpected error types
- Security scan results
- Manual periodic review of production error messages

### Performance Metrics (PERF-001)

- Test suite execution time
  - **Baseline**: <10 seconds for 22 tests
  - **Alert**: >15 seconds (investigate slowdown)
- CI pipeline duration
  - **Target**: No significant increase from story

### Operational Metrics (OPS-001, OPS-002)

- Test flakiness rate
  - **Target**: 0% flakiness over 100 runs
  - **Alert**: Any flaky test (investigate immediately)
- Test coverage percentage
  - **Target**: ≥95% overall
  - **Alert**: <95% (investigate coverage drop)
- Test failure rate in CI
  - **Target**: 0% for story tests
  - **Alert**: Any failure (investigate and fix)

### Technical Metrics (TECH-001, TECH-002, TECH-003)

- Prisma version consistency
  - **Monitor**: package.json vs test helper versions
  - **Alert**: Version mismatch detected
- Environment stub cleanup verification
  - **Monitor**: Test execution order independence
  - **Alert**: Order-dependent test failures
- React async test reliability
  - **Monitor**: waitFor() usage patterns
  - **Alert**: Timeout warnings in tests

## Risk Review Triggers

Review and update risk profile when:

### Architecture Changes

- Error handling patterns modified
- New error types introduced
- Environment configuration changes

### Dependency Updates

- Prisma client version upgrade
- Vitest version upgrade
- React Testing Library version upgrade

### Security Events

- Production error information leakage detected
- Security vulnerability discovered in error handling
- Compliance audit findings

### Performance Issues

- Test suite execution time increases significantly
- CI pipeline slows down
- Coverage collection overhead becomes problematic

### Regulatory Changes

- New compliance requirements for error handling
- Privacy regulations affecting error logging
- Security standards updates

### Test Patterns

- Flakiness detected in production tests
- Coverage drops below target
- New testing requirements identified

## Quality Gate Integration

**Gate Determination Rules**:

- Any risk score ≥ 9 (Critical) → Gate = FAIL (unless waived by security review)
- Any risk score ≥ 6 (High) → Gate = CONCERNS (requires mitigation plan)
- All risks score < 6 → Gate = PASS

**Current Gate Status**: CONCERNS

- Reason: SEC-001 (score 9) requires security validation
- Required Actions: Implement all environment tests, complete security review
- Unmitigated Risks: None after testing implementation
- Waiver Required: None (mitigation through comprehensive testing)

**Gate Progression**:

1. **Before Testing**: FAIL (SEC-001 critical, no validation)
2. **After Unit Tests**: CONCERNS (environment tests exist, need validation)
3. **After Security Review**: PASS (all mitigations verified)

## Conclusion

Story 1.9 presents a **moderate overall risk profile** (71/100 score) with **1 critical security risk** that requires careful attention. The critical risk (SEC-001: Production Error Message Information Leakage) is fully addressable through the planned test scenarios and security review process.

### Key Strengths

- Comprehensive test coverage (22 scenarios)
- Well-defined acceptance criteria
- Established testing patterns from prior stories
- Strong focus on security-critical error handling

### Key Concerns

- Production environment security is paramount
- Test helper accuracy affects reliability
- Async test flakiness requires careful implementation

### Recommendations

1. **Implement SEC-001 mitigation first** - Environment-specific tests are critical
2. **Validate test helpers early** - Ensure Prisma/ZodError helpers match reality
3. **Use established patterns** - Follow React Testing Library best practices
4. **Security review required** - Manual validation before production deployment
5. **Monitor test stability** - Track flakiness metrics in CI

### Approval Requirements

- ✅ All 22 tests implemented and passing
- ✅ Environment-specific tests validated (SEC-001)
- ✅ Test helpers verified accurate (TECH-001)
- ✅ No test flakiness detected (OPS-001)
- ✅ Security review sign-off
- ✅ Coverage target (95%+) achieved

**Overall Assessment**: Story is **safe to implement** with appropriate mitigations. Critical security risk (SEC-001) is addressable through planned testing and validation. Recommend proceeding with implementation following all mitigation strategies.
